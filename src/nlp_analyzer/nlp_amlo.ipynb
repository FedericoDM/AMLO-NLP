{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLO Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from amlo_parser import AMLOParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some classes and functions to help us with the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF to find the most important words in the text\n",
    "def get_most_important_words(text, filename, save=False):\n",
    "    # Create a TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Apply the vectorizer\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "    # Print the result\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    words = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame with the result\n",
    "    df = pd.DataFrame({\"Word\": words, \"Score\": scores})\n",
    "    df.sort_values(\"Score\", ascending=False, inplace=True)\n",
    "\n",
    "    if save:\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Word Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1241/1241 [00:37<00:00, 32.75it/s]\n"
     ]
    }
   ],
   "source": [
    "text_parser = AMLOParser(PATH)\n",
    "\n",
    "# List of all the text files\n",
    "all_files = os.listdir(PATH)\n",
    "\n",
    "# Get the president's dialogues from a text file\n",
    "new_path = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/word_scores/\"\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    if file.endswith(\".txt\"):\n",
    "        text = text_parser.get_presidents_dialogues(file, remove_stopwords=True)\n",
    "        new_file_path = os.path.join(new_path, file)\n",
    "        new_file_path = new_file_path.replace(\".txt\", \"_word_scores.csv\")\n",
    "        get_most_important_words(text, new_file_path, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labeled data\n",
    "class TrainingSet:\n",
    "    TRAINING_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/training_data/\"\n",
    "    LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\"\n",
    "    TEXT_FILES_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "    DIALOGUES_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/presidents_dialogues/\"\n",
    "\n",
    "    def __init__(self, remove_stopwords):\n",
    "        self.amlo_parser = AMLOParser(self.TEXT_FILES_PATH)\n",
    "        self.path = LABELED_PATH\n",
    "        self.labeled_data, self.agressive_phrases = self.read_labeled_data()\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "\n",
    "        self.labeled_conference_ids = self.labeled_data[\"conference_id\"].unique()\n",
    "        self.all_files = os.listdir(self.TEXT_FILES_PATH)\n",
    "\n",
    "    def read_labeled_data(self):\n",
    "        \"\"\"\n",
    "        Reads the labeled data and agressive phrases from the excel file\n",
    "        \"\"\"\n",
    "        labeled_data = pd.read_excel(self.path, sheet_name=\"labels\")\n",
    "        agressive_phrases = pd.read_excel(self.path, sheet_name=\"frases_odio\")\n",
    "\n",
    "        labeled_data = labeled_data.dropna()\n",
    "        labeled_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return labeled_data, agressive_phrases\n",
    "\n",
    "    def agressive_phrases_to_txt(self, agressive_phrases, conference_id):\n",
    "        \"\"\"\n",
    "        Parses the utterances from the dataframe and saves them to a txt file\n",
    "        \"\"\"\n",
    "        # Save the agressive phrases to a txt file\n",
    "        agressive_phrases_df = agressive_phrases.loc[\n",
    "            agressive_phrases[\"conference_id\"] == conference_id, :\n",
    "        ]\n",
    "\n",
    "        agressive_phrases_df.reset_index(drop=True, inplace=True)\n",
    "        agressive_phrases_df = agressive_phrases_df.loc[:, [\"phrase\"]]\n",
    "\n",
    "        # Write the text to a file\n",
    "        new_file_path = os.path.join(self.TRAINING_PATH, f\"{conference_id}.txt\")\n",
    "        new_file_path = new_file_path.replace(\".txt\", \"_agressive_phrases.txt\")\n",
    "\n",
    "        with open(new_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for index, row in agressive_phrases_df.iterrows():\n",
    "                phrase = self.amlo_parser.clean_text(\n",
    "                    row[\"phrase\"], remove_stopwords=self.remove_stopwords\n",
    "                )\n",
    "                f.write(phrase + \"\\n\")\n",
    "\n",
    "    def non_agressive_to_txt(self, conference_id):\n",
    "        \"\"\"\n",
    "        Copies the non-agressive phrases to a txt file. Such phrases are under the\n",
    "        president's dialogues folder\n",
    "        \"\"\"\n",
    "        # Save the non-agressive phrases to a txt file\n",
    "        print(f\"Conference {conference_id} is not agressive\")\n",
    "        # Copy file to training data\n",
    "        dialogue_path = os.path.join(\n",
    "            self.DIALOGUES_PATH, f\"{conference_id}_president_dialogues.txt\"\n",
    "        )\n",
    "\n",
    "        with open(dialogue_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            text = self.amlo_parser.clean_text(\n",
    "                text, remove_stopwords=self.remove_stopwords\n",
    "            )\n",
    "\n",
    "        new_file_path = os.path.join(\n",
    "            self.TRAINING_PATH, f\"{conference_id}_non_agressive.txt\"\n",
    "        )\n",
    "\n",
    "        with open(new_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "    def create_training_set(self):\n",
    "        for file in tqdm(self.all_files):\n",
    "            if file.endswith(\".txt\"):\n",
    "                conference_id = int(re.findall(r\"\\d+\", file)[0])\n",
    "\n",
    "                if conference_id in self.labeled_conference_ids:\n",
    "                    # Get conference label\n",
    "                    conference_label = self.labeled_data.loc[\n",
    "                        self.labeled_data[\"conference_id\"] == conference_id,\n",
    "                        \"is_agressive\",\n",
    "                    ].values[0]\n",
    "\n",
    "                    if conference_label == 1:\n",
    "                        # Get the agressive phrases\n",
    "                        self.agressive_phrases_to_txt(\n",
    "                            self.agressive_phrases, conference_id\n",
    "                        )\n",
    "                    else:\n",
    "                        # Copy the file to the training data\n",
    "                        self.non_agressive_to_txt(conference_id)\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 490/1241 [00:00<00:00, 4556.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference 20181207 is not agressive\n",
      "Conference 20190102 is not agressive\n",
      "Conference 20190111 is not agressive\n",
      "Conference 20190227 is not agressive\n",
      "Conference 20200128 is not agressive\n",
      "Conference 20210510 is not agressive\n",
      "Conference 20221125 is not agressive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1241/1241 [00:00<00:00, 5084.57it/s]\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(PATH)\n",
    "\n",
    "training_set = TrainingSet(remove_stopwords=True)\n",
    "training_set.create_training_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible models to use:\n",
    "\n",
    "Summarization:\n",
    "\n",
    "https://huggingface.co/mrm8488/bert2bert_shared-spanish-finetuned-summarization\n",
    "\n",
    "\n",
    "Sentiment Analysis:\n",
    "\n",
    "https://huggingface.co/finiteautomata/beto-sentiment-analysis\n",
    "\n",
    "\n",
    "Emotion Analysis:\n",
    "\n",
    "https://huggingface.co/finiteautomata/beto-emotion-analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
