{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLO Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from amlo_parser import AMLOParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some classes and functions to help us with the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF to find the most important words in the text\n",
    "def get_most_important_words(text, filename, save=False):\n",
    "    # Create a TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Apply the vectorizer\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "    # Print the result\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    words = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame with the result\n",
    "    df = pd.DataFrame({\"Word\": words, \"Score\": scores})\n",
    "    df.sort_values(\"Score\", ascending=False, inplace=True)\n",
    "\n",
    "    if save:\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labeled data\n",
    "class TrainingSet:\n",
    "    TRAINING_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/training_data/\"\n",
    "    LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\"\n",
    "    TEXT_FILES_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "    DIALOGUES_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/presidents_dialogues/\"\n",
    "\n",
    "    def __init__(self, remove_stopwords):\n",
    "        self.amlo_parser = AMLOParser(self.TEXT_FILES_PATH)\n",
    "        self.path = LABELED_PATH\n",
    "        self.labeled_data, self.agressive_phrases = self.read_labeled_data()\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "\n",
    "        self.labeled_conference_ids = self.labeled_data[\"conference_id\"].unique()\n",
    "        self.all_files = os.listdir(self.TEXT_FILES_PATH)\n",
    "\n",
    "    def read_labeled_data(self):\n",
    "        \"\"\"\n",
    "        Reads the labeled data and agressive phrases from the excel file\n",
    "        \"\"\"\n",
    "        labeled_data = pd.read_excel(self.path, sheet_name=\"labels\")\n",
    "        agressive_phrases = pd.read_excel(self.path, sheet_name=\"frases_odio\")\n",
    "\n",
    "        labeled_data = labeled_data.dropna()\n",
    "        labeled_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return labeled_data, agressive_phrases\n",
    "\n",
    "    def agressive_phrases_to_txt(self, agressive_phrases, conference_id):\n",
    "        \"\"\"\n",
    "        Parses the utterances from the dataframe and saves them to a txt file\n",
    "        \"\"\"\n",
    "        # Save the agressive phrases to a txt file\n",
    "        agressive_phrases_df = agressive_phrases.loc[\n",
    "            agressive_phrases[\"conference_id\"] == conference_id, :\n",
    "        ]\n",
    "\n",
    "        agressive_phrases_df.reset_index(drop=True, inplace=True)\n",
    "        agressive_phrases_df = agressive_phrases_df.loc[:, [\"phrase\"]]\n",
    "\n",
    "        # Write the text to a file\n",
    "        new_file_path = os.path.join(self.TRAINING_PATH, f\"{conference_id}.txt\")\n",
    "        new_file_path = new_file_path.replace(\".txt\", \"_agressive_phrases.txt\")\n",
    "\n",
    "        with open(new_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for index, row in agressive_phrases_df.iterrows():\n",
    "                phrase = self.amlo_parser.clean_text(\n",
    "                    row[\"phrase\"], remove_stopwords=self.remove_stopwords\n",
    "                )\n",
    "                f.write(phrase + \"\\n\")\n",
    "\n",
    "    def non_agressive_to_txt(self, conference_id):\n",
    "        \"\"\"\n",
    "        Copies the non-agressive phrases to a txt file. Such phrases are under the\n",
    "        president's dialogues folder\n",
    "        \"\"\"\n",
    "        # Save the non-agressive phrases to a txt file\n",
    "        print(f\"Conference {conference_id} is not agressive\")\n",
    "        # Copy file to training data\n",
    "        dialogue_path = os.path.join(\n",
    "            self.DIALOGUES_PATH, f\"{conference_id}_president_dialogues.txt\"\n",
    "        )\n",
    "\n",
    "        with open(dialogue_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            text = self.amlo_parser.clean_text(\n",
    "                text, remove_stopwords=self.remove_stopwords\n",
    "            )\n",
    "\n",
    "        new_file_path = os.path.join(\n",
    "            self.TRAINING_PATH, f\"{conference_id}_non_agressive.txt\"\n",
    "        )\n",
    "\n",
    "        with open(new_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "    def create_training_set(self):\n",
    "        for file in tqdm(self.all_files):\n",
    "            if file.endswith(\".txt\"):\n",
    "                conference_id = int(re.findall(r\"\\d+\", file)[0])\n",
    "\n",
    "                if conference_id in self.labeled_conference_ids:\n",
    "                    # Get conference label\n",
    "                    conference_label = self.labeled_data.loc[\n",
    "                        self.labeled_data[\"conference_id\"] == conference_id,\n",
    "                        \"is_agressive\",\n",
    "                    ].values[0]\n",
    "\n",
    "                    if conference_label == 1:\n",
    "                        # Get the agressive phrases\n",
    "                        self.agressive_phrases_to_txt(\n",
    "                            self.agressive_phrases, conference_id\n",
    "                        )\n",
    "                    else:\n",
    "                        # Copy the file to the training data\n",
    "                        self.non_agressive_to_txt(conference_id)\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 266/1241 [00:00<00:00, 2450.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference 20181207 is not agressive\n",
      "Conference 20190102 is not agressive\n",
      "Conference 20190111 is not agressive\n",
      "Conference 20190227 is not agressive\n",
      "Conference 20200128 is not agressive\n",
      "Conference 20210510 is not agressive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1241/1241 [00:00<00:00, 4295.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference 20221125 is not agressive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(PATH)\n",
    "\n",
    "training_set = TrainingSet(remove_stopwords=True)\n",
    "training_set.create_training_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSION TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_training_df():\n",
    "    \"\"\"\n",
    "    Creates a DataFrame with the training data for the classification model\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Specify the path to your training data folder\n",
    "    folder_path = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/training_data/\"\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            # Determine the label based on the file name\n",
    "            label = 0 if \"non\" in file else 1\n",
    "            id = int(re.findall(r\"\\d+\", file)[0])\n",
    "            # Read the text file\n",
    "            with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            data.append({\"id\": id, \"text\": text, \"label\": label})\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(df):\n",
    "    \"\"\"\n",
    "    Trains a simple classification model using TF-IDF and logistic regression\n",
    "    \"\"\"\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "    # Fit and transform the 'text' column\n",
    "    X = tfidf_vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Assuming 'label' is your target variable\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize and train the logistic regression model\n",
    "    model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print out the evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let us do it for the entire dataset\n",
    "\n",
    "data = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for file in os.listdir(training_set.DIALOGUES_PATH):\n",
    "    if file.endswith(\".txt\"):\n",
    "        # Determine the label based on the file name\n",
    "        conference_id = int(re.findall(r\"\\d+\", file)[0])\n",
    "\n",
    "        if conference_id not in training_set.labeled_conference_ids:\n",
    "            # Read the text file\n",
    "            with open(\n",
    "                os.path.join(training_set.DIALOGUES_PATH, file), \"r\", encoding=\"utf-8\"\n",
    "            ) as f:\n",
    "                text = f.read()\n",
    "                data.append({\"id\": conference_id, \"text\": text})\n",
    "\n",
    "\n",
    "unseen_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20181210</td>\n",
       "      <td>buenas tardes sí son buenos días llegué tarde ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20181211</td>\n",
       "      <td>caso sindicatos hemos hecho compromiso impulsa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20181212</td>\n",
       "      <td>buenos días hoy vamos presentarles plan genera...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20181217</td>\n",
       "      <td>buenos díasestamos iniciando semana terminamos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20181218</td>\n",
       "      <td>buenos días informo haya dudas falta informaci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>20240212</td>\n",
       "      <td>buenos días ánimo bueno vamos iniciar semana t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>20240213</td>\n",
       "      <td>buenos días ánimo ánimo pues vamos informar ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>20240214</td>\n",
       "      <td>buenos días ánimo\\nbien bien mejor mejor ayer ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>20240215</td>\n",
       "      <td>da mucho gusto estar nuevo aquí ustedes acapul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>20240216</td>\n",
       "      <td>buenos días ánimo bueno vamos informar hoy jui...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1212 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  label\n",
       "0     20181210  buenas tardes sí son buenos días llegué tarde ...      0\n",
       "1     20181211  caso sindicatos hemos hecho compromiso impulsa...      0\n",
       "2     20181212  buenos días hoy vamos presentarles plan genera...      0\n",
       "3     20181217  buenos díasestamos iniciando semana terminamos...      0\n",
       "4     20181218  buenos días informo haya dudas falta informaci...      0\n",
       "...        ...                                                ...    ...\n",
       "1207  20240212  buenos días ánimo bueno vamos iniciar semana t...      0\n",
       "1208  20240213  buenos días ánimo ánimo pues vamos informar ho...      0\n",
       "1209  20240214  buenos días ánimo\\nbien bien mejor mejor ayer ...      0\n",
       "1210  20240215  da mucho gusto estar nuevo aquí ustedes acapul...      0\n",
       "1211  20240216  buenos días ánimo bueno vamos informar hoy jui...      0\n",
       "\n",
       "[1212 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the labels for the unseen data\n",
    "\n",
    "# Transform the new texts\n",
    "X_new = tfidf_vectorizer.transform(unseen_df[\"text\"])\n",
    "\n",
    "# Make predictions\n",
    "y_new = model.predict(X_new)\n",
    "\n",
    "# Add the predictions to the DataFrame\n",
    "unseen_df[\"label\"] = y_new\n",
    "\n",
    "# Print the results\n",
    "unseen_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                               text  label\n",
      "38    20190211  buenos días día hoy vamos dar conocer venido s...      1\n",
      "536   20210303  buenos díastardes alargó mucho reunión tenemos...      1\n",
      "538   20210305  buenos días estamos finalizando semana vamos g...      1\n",
      "552   20210326  buenos días bueno vamos iniciar hoy vamos gira...      1\n",
      "578   20210507  buenos días bueno vamos informar vio ayer fina...      1\n",
      "675   20211006  buenos días ánimo bueno pues día hoy muy impor...      1\n",
      "694   20211105  buenos días está haciendo frío bueno ayer acor...      1\n",
      "725   20211229  buenos días vamos informar hoy quién quién men...      1\n",
      "784   20220401  buenos días bueno pues vamos dedicar día conte...      1\n",
      "789   20220408  muy buenos días hablemos más vámonos preguntas...      1\n",
      "963   20221228  buenos días ánimo muy bien empezaron poner muñ...      1\n",
      "983   20230203  buenos días ánimo bueno pues vamos informar do...      1\n",
      "995   20230221  buenos días ánimo bueno día hoy vamos informar...      1\n",
      "996   20230222  buenos días hizo tarde ánimo bueno pues hoy mi...      1\n",
      "999   20230227  buenos días ánimo bueno pues iniciamos semana ...      1\n",
      "1003  20230303  muy buenos días ánimo viernes hay ir giras vam...      1\n",
      "1034  20230428  buenos días ánimo da mucho gusto estar nuevo a...      1\n",
      "1049  20230519  buenos días ánimo ánimo ánimo pues día hoy ten...      1\n",
      "1069  20230616  buenos díastardes ánimo vamos informar termina...      1\n",
      "1131  20230929  buenos díastardes ánimo hay buenas noticias: t...      1\n",
      "1143  20231019  buenos días ánimo cómo están empezó frio\\nsí s...      1\n",
      "1202  20240131  buenos días felicidades ánimo\\nestán aquí muy ...      1\n"
     ]
    }
   ],
   "source": [
    "print(unseen_df.loc[unseen_df[\"label\"] == 1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible models to use:\n",
    "\n",
    "Summarization:\n",
    "\n",
    "https://huggingface.co/mrm8488/bert2bert_shared-spanish-finetuned-summarization\n",
    "\n",
    "\n",
    "Sentiment Analysis:\n",
    "\n",
    "https://huggingface.co/finiteautomata/beto-sentiment-analysis\n",
    "\n",
    "\n",
    "Emotion Analysis:\n",
    "\n",
    "https://huggingface.co/finiteautomata/beto-emotion-analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
