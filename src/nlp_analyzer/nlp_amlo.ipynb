{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLO Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from amlo_parser import AMLOParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some classes and functions to help us with the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF to find the most important words in the text\n",
    "def get_most_important_words(text, filename, save=False):\n",
    "    # Create a TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Apply the vectorizer\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "    # Print the result\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    words = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame with the result\n",
    "    df = pd.DataFrame({\"Word\": words, \"Score\": scores})\n",
    "    df.sort_values(\"Score\", ascending=False, inplace=True)\n",
    "\n",
    "    if save:\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Word Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1241 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m new_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(new_path, file)\n\u001b[0;32m     13\u001b[0m new_file_path \u001b[38;5;241m=\u001b[39m new_file_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_word_scores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mget_most_important_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mget_most_important_words\u001b[1;34m(text, filename, save)\u001b[0m\n\u001b[0;32m      4\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Apply the vectorizer\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the result\u001b[39;00m\n\u001b[0;32m     10\u001b[0m scores \u001b[38;5;241m=\u001b[39m tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\fdmol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fdmol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fdmol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "text_parser = AMLOParser(PATH)\n",
    "\n",
    "# List of all the text files\n",
    "all_files = os.listdir(PATH)\n",
    "\n",
    "# Get the president's dialogues from a text file\n",
    "new_path = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/word_scores/\"\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    if file.endswith(\".txt\"):\n",
    "        text = text_parser.get_presidents_dialogues(file, remove_stopwords=True)\n",
    "        new_file_path = os.path.join(new_path, file)\n",
    "        new_file_path = new_file_path.replace(\".txt\", \"_word_scores.csv\")\n",
    "        get_most_important_words(text, new_file_path, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labeled data\n",
    "class TrainingSet:\n",
    "    TRAINING_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/training_data/\"\n",
    "    LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\"\n",
    "    TEXT_FILES_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "    DIALOGUES_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/presidents_dialogues/\"\n",
    "\n",
    "    def __init__(self, remove_stopwords):\n",
    "        self.amlo_parser = AMLOParser(self.TEXT_FILES_PATH)\n",
    "        self.path = LABELED_PATH\n",
    "        self.labeled_data, self.agressive_phrases = self.read_labeled_data()\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "\n",
    "        self.labeled_conference_ids = self.labeled_data[\"conference_id\"].unique()\n",
    "        self.all_files = os.listdir(self.TEXT_FILES_PATH)\n",
    "\n",
    "    def read_labeled_data(self):\n",
    "        \"\"\"\n",
    "        Reads the labeled data and agressive phrases from the excel file\n",
    "        \"\"\"\n",
    "        labeled_data = pd.read_excel(self.path, sheet_name=\"labels\")\n",
    "        agressive_phrases = pd.read_excel(self.path, sheet_name=\"frases_odio\")\n",
    "\n",
    "        labeled_data = labeled_data.dropna()\n",
    "        labeled_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return labeled_data, agressive_phrases\n",
    "\n",
    "    def agressive_phrases_to_txt(self, agressive_phrases, conference_id):\n",
    "        \"\"\"\n",
    "        Parses the utterances from the dataframe and saves them to a txt file\n",
    "        \"\"\"\n",
    "        # Save the agressive phrases to a txt file\n",
    "        agressive_phrases_df = agressive_phrases.loc[\n",
    "            agressive_phrases[\"conference_id\"] == conference_id, :\n",
    "        ]\n",
    "\n",
    "        agressive_phrases_df.reset_index(drop=True, inplace=True)\n",
    "        agressive_phrases_df = agressive_phrases_df.loc[:, [\"phrase\"]]\n",
    "\n",
    "        # Write the text to a file\n",
    "        new_file_path = os.path.join(self.TRAINING_PATH, f\"{conference_id}.txt\")\n",
    "        new_file_path = new_file_path.replace(\".txt\", \"_agressive_phrases.txt\")\n",
    "\n",
    "        with open(new_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for index, row in agressive_phrases_df.iterrows():\n",
    "                phrase = self.amlo_parser.clean_text(\n",
    "                    row[\"phrase\"], remove_stopwords=self.remove_stopwords\n",
    "                )\n",
    "                f.write(phrase + \"\\n\")\n",
    "\n",
    "    def non_agressive_to_txt(self, conference_id):\n",
    "        \"\"\"\n",
    "        Copies the non-agressive phrases to a txt file. Such phrases are under the\n",
    "        president's dialogues folder\n",
    "        \"\"\"\n",
    "        # Save the non-agressive phrases to a txt file\n",
    "        print(f\"Conference {conference_id} is not agressive\")\n",
    "        # Copy file to training data\n",
    "        dialogue_path = os.path.join(\n",
    "            self.DIALOGUES_PATH, f\"{conference_id}_president_dialogues.txt\"\n",
    "        )\n",
    "\n",
    "        with open(dialogue_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            text = self.amlo_parser.clean_text(\n",
    "                text, remove_stopwords=self.remove_stopwords\n",
    "            )\n",
    "\n",
    "        new_file_path = os.path.join(\n",
    "            self.TRAINING_PATH, f\"{conference_id}_non_agressive.txt\"\n",
    "        )\n",
    "\n",
    "        with open(new_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "    def create_training_set(self):\n",
    "        for file in tqdm(self.all_files):\n",
    "            if file.endswith(\".txt\"):\n",
    "                conference_id = int(re.findall(r\"\\d+\", file)[0])\n",
    "\n",
    "                if conference_id in self.labeled_conference_ids:\n",
    "                    # Get conference label\n",
    "                    conference_label = self.labeled_data.loc[\n",
    "                        self.labeled_data[\"conference_id\"] == conference_id,\n",
    "                        \"is_agressive\",\n",
    "                    ].values[0]\n",
    "\n",
    "                    if conference_label == 1:\n",
    "                        # Get the agressive phrases\n",
    "                        self.agressive_phrases_to_txt(\n",
    "                            self.agressive_phrases, conference_id\n",
    "                        )\n",
    "                    else:\n",
    "                        # Copy the file to the training data\n",
    "                        self.non_agressive_to_txt(conference_id)\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 490/1241 [00:00<00:00, 4556.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference 20181207 is not agressive\n",
      "Conference 20190102 is not agressive\n",
      "Conference 20190111 is not agressive\n",
      "Conference 20190227 is not agressive\n",
      "Conference 20200128 is not agressive\n",
      "Conference 20210510 is not agressive\n",
      "Conference 20221125 is not agressive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1241/1241 [00:00<00:00, 5084.57it/s]\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(PATH)\n",
    "\n",
    "training_set = TrainingSet(remove_stopwords=True)\n",
    "training_set.create_training_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible models to use:\n",
    "\n",
    "Summarization:\n",
    "\n",
    "https://huggingface.co/mrm8488/bert2bert_shared-spanish-finetuned-summarization\n",
    "\n",
    "\n",
    "Sentiment Analysis:\n",
    "\n",
    "https://huggingface.co/finiteautomata/beto-sentiment-analysis\n",
    "\n",
    "\n",
    "Emotion Analysis:\n",
    "\n",
    "https://huggingface.co/finiteautomata/beto-emotion-analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
