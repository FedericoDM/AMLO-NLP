{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLO Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "from amlo_parser import AMLOParser\n",
    "from training_set import TrainingSet\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Local imports\n",
    "from xgb_model import XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training set, along with its correspoding txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1246/1246 [00:00<00:00, 6523.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference 20181207 is not agressive\n",
      "Conference 20190102 is not agressive\n",
      "Conference 20190111 is not agressive\n",
      "Conference 20190227 is not agressive\n",
      "Conference 20200128 is not agressive\n",
      "Conference 20210510 is not agressive\n",
      "Conference 20221125 is not agressive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(PATH)\n",
    "\n",
    "training_set = TrainingSet(remove_stopwords=True)\n",
    "training_set.create_training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your training data folder\n",
    "folder_path = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/training_data/\"\n",
    "\n",
    "param = {\n",
    "    \"max_depth\": 8,\n",
    "    \"eta\": 0.15,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = XGBoost(\n",
    "    folder_path=folder_path,\n",
    "    dialogues_path=training_set.DIALOGUES_PATH,\n",
    "    xgb_params=param,\n",
    ")\n",
    "\n",
    "\n",
    "xgb_model.create_regression_training_df()\n",
    "xgb_model.create_unseen_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = xgb_model.training_df\n",
    "unseen_df = xgb_model.unseen_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to implement a NNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(texts, tokenizer, min_freq=1):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from the given texts based on frequency.\n",
    "\n",
    "    Args:\n",
    "    - texts (list of str): List of text samples.\n",
    "    - tokenizer (callable): Function to tokenize text.\n",
    "    - min_freq (int): Minimum frequency for a word to be included in the vocab.\n",
    "\n",
    "    Returns:\n",
    "    - vocab (dict): Mapping of word to unique index.\n",
    "    \"\"\"\n",
    "    # Tokenize all texts and count word frequencies\n",
    "    counter = Counter(token for text in texts for token in tokenizer(text))\n",
    "\n",
    "    # Filter words by min_freq and assign unique indices\n",
    "    vocab = {\n",
    "        word: i + 2\n",
    "        for i, (word, freq) in enumerate(counter.items())\n",
    "        if freq >= min_freq\n",
    "    }  # Start indexing from 2\n",
    "\n",
    "    # Special tokens\n",
    "    vocab[\"<pad>\"] = 0  # Padding token\n",
    "    vocab[\"<unk>\"] = 1  # Unknown word token\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Example tokenizer function\n",
    "def tokenizer(text):\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggressivityDataset(Dataset):\n",
    "    def __init__(self, texts, vocab, scores=None):\n",
    "        \"\"\"\n",
    "        texts: List of text data\n",
    "        vocab: A dictionary mapping tokens to indices\n",
    "        scores: List of aggressivity scores (for training data); None for unseen data\n",
    "        \"\"\"\n",
    "        self.texts = [self.numericalize(text, vocab) for text in texts]\n",
    "        self.scores = scores\n",
    "\n",
    "    def numericalize(self, text, vocab):\n",
    "        # Simple tokenization and numericalization based on the provided vocab\n",
    "        tokenized = (\n",
    "            text.lower().split()\n",
    "        )  # Simple whitespace tokenization, adjust as needed\n",
    "        return [\n",
    "            vocab.get(token, 0) for token in tokenized\n",
    "        ]  # 0 as the index for unknown words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.scores is not None:\n",
    "            return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(\n",
    "                self.scores[idx], dtype=torch.float\n",
    "            )\n",
    "        return (\n",
    "            torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            0,\n",
    "        )  # Return 0 as a placeholder score for unseen data\n",
    "\n",
    "\n",
    "# Example usage (assuming 'vocab' is your vocabulary dictionary mapping words to indices):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab from your training_df['text']\n",
    "vocab = build_vocab(training_df[\"text\"].tolist(), tokenizer)\n",
    "\n",
    "training_dataset = AggressivityDataset(\n",
    "    training_df[\"text\"].tolist(), vocab, training_df[\"score\"].tolist()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
