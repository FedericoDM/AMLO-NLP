{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLO Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "from amlo_parser import AMLOParser\n",
    "from training_set import TrainingSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/text_files/\"\n",
    "LABELED_PATH = \"C:/Users/fdmol/Desktop/AMLO-NLP/src/data/amlo_labeling.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training set, along with its correspoding txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1246/1246 [00:00<00:00, 6523.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference 20181207 is not agressive\n",
      "Conference 20190102 is not agressive\n",
      "Conference 20190111 is not agressive\n",
      "Conference 20190227 is not agressive\n",
      "Conference 20200128 is not agressive\n",
      "Conference 20210510 is not agressive\n",
      "Conference 20221125 is not agressive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(PATH)\n",
    "\n",
    "training_set = TrainingSet(remove_stopwords=True)\n",
    "training_set.create_training_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training for scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to implement a NNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, scores):\n",
    "        self.texts = texts\n",
    "        self.scores = scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"text\": self.texts[idx], \"score\": self.scores[idx]}\n",
    "\n",
    "\n",
    "def numericalize(text):\n",
    "    return [vocab[word] for word in tokenizer(text) if word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming create_regression_training_df and create_unseen_df methods are called before\n",
    "# Tokenize and numericalize text data\n",
    "tokenizer = lambda x: x.split()  # Simple tokenizer\n",
    "vocab = Counter()\n",
    "\n",
    "for text in training_df[\"text\"]:\n",
    "    vocab.update(tokenizer(text))\n",
    "\n",
    "vocab = {\n",
    "    word: i + 1 for i, word in enumerate(vocab)\n",
    "}  # Mapping word to index (starting from 1)\n",
    "\n",
    "\n",
    "training_df[\"text\"] = training_df[\"text\"].apply(numericalize)\n",
    "unseen_df[\"text\"] = unseen_df[\"text\"].apply(numericalize)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = TextDataset(\n",
    "    torch.tensor(list(training_df[\"text\"]), dtype=torch.long),\n",
    "    torch.tensor(training_df[\"score\"].values, dtype=torch.float),\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
